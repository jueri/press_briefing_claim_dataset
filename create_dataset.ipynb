{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the SMC claim dataset\n",
    "This notebook guides through the dataset creation process. The press briefings are scraped from the SMC website, parsed into structured texts, enriched with topics, and imported into an SQLite database.\n",
    "\n",
    "**Table of content:**\n",
    "1. Scrape data\n",
    "2. Create database\n",
    "3. Parse data\n",
    "4. Split sentences\n",
    "5. Sentence Wikification\n",
    "6. Topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from config import BASE_DIR, BASE_URL, DB_PATH, METADATA_PATH\n",
    "from src import create_db, load_data, parse_pdf, wikify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(os.listdir(os.path.join(BASE_DIR, \"pdf\"))) > 0:\n",
    "    print(\"--- Scraping Press Briefing Data ---\")\n",
    "\n",
    "    pressbriefing_urls = load_data.extrect_all_pressbriefing_links()  # load all urls\n",
    "\n",
    "    # Save PDFs\n",
    "    results = []\n",
    "    for url in pressbriefing_urls:\n",
    "        responds = requests.get(url)\n",
    "        bs = BeautifulSoup(responds.content, \"html.parser\")\n",
    "        try:\n",
    "            introduction = load_data.extrect_introduction(bs)\n",
    "        except:\n",
    "            introduction = None\n",
    "            print(\"ERROR: Could not load introduction:\", url)\n",
    "        try:\n",
    "            pdf_url = load_data.extrect_pdf_url(bs)\n",
    "        except:\n",
    "            pdf_path = None\n",
    "            print(\"ERROR: Could not load pdf path:\", url)\n",
    "        try:\n",
    "            pdf_path = load_data.load_pdf_from_url(pdf_url, os.path.join(\"data\", \"SMC_dataset\", \"pdf\"))\n",
    "        except:\n",
    "            pdf_path = None\n",
    "            print(\"ERROR: Could not load pdf:\", url)\n",
    "\n",
    "\n",
    "        pb = {\n",
    "            \"introduction\": introduction,\n",
    "            \"pdf_path\": pdf_path,\n",
    "            \"pdf_url\": pdf_url,\n",
    "            \"url\": url\n",
    "        }\n",
    "        results.append(pb)\n",
    "        time.sleep(random.randint(1, 7))  # random sleep\n",
    "\n",
    "    # Save metadata.csv\n",
    "    df = pd.DataFrame(results)\n",
    "    df[\"pdf_url\"] = df.apply(lambda x: x[\"pdf_url\"] if x[\"pdf_url\"].startswith(\"http\") else BASE_URL + x[\"pdf_url\"], axis = 1)\n",
    "    df[\"pdf_url\"] = df.apply(lambda x: None if x[\"pdf_url\"] == BASE_URL else x[\"pdf_url\"], axis=1)\n",
    "    df.to_csv(METADATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DB_PATH):\n",
    "    db = create_db.create_connection(DB_PATH)\n",
    "    create_db.create_tables(db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "metadata = metadata.dropna().reset_index(drop=True)  # delete na rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\n",
    "    \"Transkript_Versorgungssituation_Krankenhaeuser_SMC-virtuelles-Press-Briefing_20202_03_11.pdf\",\n",
    "    \"Transkript_vPB_Wasserstoffstrategie.pdf\",\n",
    "    \"Trankript_Duerre-Landwirtschaft-Waelder_SMC-Press-Briefing_2020-05-05.pdf\",\n",
    "    \"Transkript_Corona-und-Klima_SMC-Press-Briefing_2020-04-16.pdf\",\n",
    "    \"Transkript_virPB_Kinder_COVID.pdf\",\n",
    "    \"Transkript_Heinsberg-Studie_Ergebnisse_SMC-Press-Briefing_2020-05-04.pdf\",\n",
    "    \"Transkript_Atomenergie-und-Klimawandel_SMC-PressBriefing_2020-02-26.pdf\",\n",
    "    \"Transkript_SMC_Press_Briefing_Machine_Learning_Medizin_180518.pdf\",\n",
    "    \"Transkript_gesundeStaedte_vPressBriefing_30112020.pdf\",\n",
    "    \"Transkript_CO2-Emissionen-im-Corona-Jahr_SMC-Press-Briefing_2020-12-10.pdf\",\n",
    "    \"Transkript_vPB_Mutationen_SARSCoV2.pdf\",\n",
    "    \"Transkript_Die-_neue-GAP_SMC-Press-Briefing_20210316.pdf\",\n",
    "    \"Transkript_Modellierungen_COVID_SMC_virutelles_Press-Briefing_07-05-2020.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in metadata.iterrows():\n",
    "    # add metadata\n",
    "    pdf_path = pdf[1][\"pdf_path\"]\n",
    "    pdf_url = pdf[1][\"pdf_url\"]\n",
    "    introduction_text = pdf[1][\"introduction\"]\n",
    "    \n",
    "    if pdf_path.replace(\"data/SMC_dataset/pdf/\", \"\") in exclude:  # exclude some pdfs\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # read pdf file\n",
    "    head, body = parse_pdf.read_pdf(\"../\" + pdf[1][\"pdf_path\"])  # parse pdf\n",
    "    fulltext = \" \".join(body)\n",
    "    fulltext_clean = parse_pdf.sanetize(fulltext)\n",
    "\n",
    "    # parse head\n",
    "    head_metadata = parse_pdf.parse_head(head)\n",
    "    title = head_metadata.get(\"title\")\n",
    "    date = head_metadata.get(\"date\")\n",
    "    video_url = head_metadata.get(\"video_url\")\n",
    "\n",
    "    # parse body\n",
    "    segments = parse_pdf.parse_body(body)\n",
    "    persons = list(set([part.get(\"speaker\") for part in segments]))\n",
    "\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    # insert parsed press briefing\n",
    "    connection = create_db.create_connection(DB_PATH)\n",
    "    cur = connection.cursor()\n",
    "\n",
    "    # instert press briefing\n",
    "    command = (\"\"\"\n",
    "    INSERT INTO Press_Briefing(\n",
    "        pdf_path, pdf_url, introduction_text, fulltext, fulltext_clean, title, date, video_url)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\")\n",
    "    cur.execute(command, (pdf_path, pdf_url, introduction_text, fulltext, fulltext_clean, title, date, video_url))\n",
    "    connection.commit()\n",
    "\n",
    "    pb_ID = cur.execute(\"SELECT last_insert_rowid()\").fetchone()[0]  # get pb ID\n",
    "\n",
    "    # insert Person\n",
    "    person_ids = {}\n",
    "    for person in persons:\n",
    "        if person:\n",
    "            person_ID = cur.execute(\"SELECT person_ID FROM Person WHERE name=?\", (person,)).fetchone()  # check if person exist\n",
    "            if person_ID:\n",
    "                cur.execute(\"INSERT INTO is_guest (pb_ID, person_ID) VALUES (?, ?)\", (pb_ID, person_ID[0]))\n",
    "                person_ids[person] = person_ID[0]\n",
    "            else:\n",
    "                cur.execute(\"INSERT INTO Person (name) VALUES (?)\", (person,))  # add person\n",
    "                person_ID = cur.execute(\"SELECT last_insert_rowid()\").fetchone()[0]  # get person id\n",
    "                cur.execute(\"INSERT INTO is_guest (pb_ID, person_ID) VALUES (?, ?)\", (pb_ID, person_ID))\n",
    "                person_ids[person] = person_ID\n",
    "            connection.commit()\n",
    "    \n",
    "    # insert segments\n",
    "    for segment in segments:\n",
    "\n",
    "        if segment.get(\"text\"):  # db constarint\n",
    "            cur.execute(\"INSERT INTO Segment (pb_ID, speaker, text, timecode) VALUES (?, ?, ?, ?)\", (pb_ID, person_ids.get(segment.get(\"speaker\")), segment.get(\"text\"), segment.get(\"timecode\")))\n",
    "            connection.commit()\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_db.create_connection(DB_PATH)\n",
    "cur = connection.cursor()\n",
    "segments = cur.execute(\"SELECT * FROM Segment\").fetchall()\n",
    "\n",
    "for segment in segments:\n",
    "    sentences = sent_tokenize(segment[2])\n",
    "    for sentence in sentences:\n",
    "        cur.execute(\"INSERT INTO Sentence (segment_ID, pb_ID, sentence) VALUES (?, ?, ?)\", (segment[0], segment[1], sentence))\n",
    "        \n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sentence Wikification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token = os.environ.get(\"DANDELION_TOKEN\")\n",
    "token = os.environ.get(\"TAGME_TOKEN\")\n",
    "\n",
    "# Save file with ids already wikifyed\n",
    "with open(\"wikifyed.txt\", \"r\") as save_file:\n",
    "    done_ids = []\n",
    "    for done_id in save_file.readlines():\n",
    "        done_ids.append(int(done_id.replace(\"\\n\", \"\")))\n",
    "\n",
    "connection = create_db.create_connection(DB_PATH)  # DB connection\n",
    "cur = connection.cursor()\n",
    "\n",
    "sentences = cur.execute(\"SELECT sentence, sentence_ID FROM Sentence\").fetchall()\n",
    "\n",
    "\n",
    "with open(\"wikifyed.txt\", \"a\") as save_file:\n",
    "    for sentence in sentences:\n",
    "        if sentence[1] not in done_ids:\n",
    "            concepts = wikify.wifify(sentence[0], service=\"tagme\", token=token)  # wikify\n",
    "            if concepts != \"Error\":\n",
    "                if concepts:\n",
    "                    for concept in concepts:\n",
    "                        cur.execute(\"INSERT INTO Sentence_Wikification (sentence_ID, term, wiki_num, confidence, url) VALUES (?, ?, ?, ?, ?)\", (sentence[1], concept.get(\"title\"), concept.get(\"id\"), concept.get(\"link_probability\"), concept.get(\"uri\")))\n",
    "                    connection.commit()\n",
    "                save_file.write(\"\\n\"+str(sentence[1]))  # save ids\n",
    "            else:\n",
    "                connection.close()\n",
    "                break\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Topic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ.get(\"DANDELION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title topic\n",
    "connection = create_db.create_connection(DB_PATH)  # DB connection\n",
    "cur = connection.cursor()\n",
    "\n",
    "title_all = cur.execute(\"SELECT pb_ID, title FROM Press_Briefing\").fetchall()  # get all titles\n",
    "\n",
    "for title in title_all:\n",
    "    concepts = wikify.wifify(title[1], service=\"dandaleon\", token=token)  # wikify\n",
    "    if concepts:\n",
    "        for concept in concepts:\n",
    "            cur.execute(\"INSERT INTO pb_Wikification_title (pb_ID, term, wiki_num, confidence, url) VALUES (?, ?, ?, ?, ?)\", \n",
    "            (title[0], concept.get(\"title\"), concept.get(\"id\"), concept.get(\"confidence\"), concept.get(\"uri\")))\n",
    "        connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction topic\n",
    "connection = create_db.create_connection(DB_PATH)  # DB connection\n",
    "cur = connection.cursor()\n",
    "\n",
    "introductions = cur.execute(\"SELECT pb_ID, introduction_text FROM Press_Briefing\").fetchall()\n",
    "\n",
    "for introduction in introductions:\n",
    "    text = introduction[1][:2000].replace(\"https://www.sciencemediacenter.de/alle-angebote\", \"\").replace(\"\\xa0\", \"\").replace(\"\\n\", \"\")\n",
    "    concepts = wikify.detect_main_concept(text, num_entetys=5, token=token)  # wikify\n",
    "    if concepts:\n",
    "        for concept in concepts:\n",
    "            cur.execute(\"INSERT INTO pb_Wikification_intro (pb_ID, wiki_num, confidence, url) VALUES (?, ?, ?, ?)\",\n",
    "            (introduction[0], concept.get(\"id\"), concept.get(\"confidence\"), concept.get(\"uri\")))\n",
    "        connection.commit()   \n",
    "    else:\n",
    "        print(introduction[0])\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
